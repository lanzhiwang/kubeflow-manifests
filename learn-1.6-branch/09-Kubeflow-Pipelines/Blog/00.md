# Getting started with Kubeflow Pipelines

* https://cloud.google.com/blog/products/ai-machine-learning/getting-started-kubeflow-pipelines

While model construction and training are essential steps to building useful machine learning-driven applications, they comprise only a small part of what you need to pay attention to when building machine learning (ML) workflows. To address that need, last week [we announced AI Hub and Kubeflow Pipelines](https://cloud.google.com/blog/products/ai-machine-learning/introducing-ai-hub-and-kubeflow-pipelines-making-ai-simpler-faster-and-more-useful-for-businesses)—tools designed with not only data scientists, but software and data engineers in mind—that help you build and share models and ML workflows within your organization and across teams, for integration into different parts of your business. Pipelines enable you to port your data to an accessible format and location, perform data cleaning and feature engineering, analyze your trained models, version your models, scalably serve your trained models while avoiding training or serving skew, and more. As your and your machine learning team’s expertise grows, you’ll find that these workflows need to be portable and consistently repeatable, yet have many ‘moving parts’ that need to be integrated.
虽然模型构建和训练是构建有用的机器学习驱动应用程序的重要步骤，但它们仅占构建机器学习 (ML) 工作流程时需要注意的一小部分。 为了满足这一需求，我们上周发布了 AI Hub 和 Kubeflow Pipelines，这些工具不仅是为数据科学家设计的，而且还考虑到了软件和数据工程师的需求，可帮助您在组织内和跨团队构建和共享模型和机器学习工作流程，以进行集成 到您业务的不同部分。 管道使您能够将数据移植到可访问的格式和位置，执行数据清理和特征工程，分析经过训练的模型，版本化模型，可扩展地为经过训练的模型提供服务，同时避免训练或服务偏差等等。 随着您和您的机器学习团队的专业知识不断增长，您会发现这些工作流程需要可移植且始终可重复，但有许多“移动部件”需要集成。

Furthermore, most of these activities recur across multiple workflows, perhaps with only different parameter sets. Sometimes, you’ll run a set of experiments that needs to be performed in an auditable and repeatable manner. Other times, part or all of an ML workflow needs to run on-prem, but in still other contexts, it may be more productive to use managed cloud services, which make it easy to distribute and scale out the workflow steps, and to run multiple experiments in parallel.
此外，大多数这些活动在多个工作流程中重复出现，可能仅具有不同的参数集。 有时，您将运行一组需要以可审核和可重复的方式执行的实验。 其他时候，部分或全部 ML 工作流程需要在本地运行，但在其他情况下，使用托管云服务可能会更高效，这使得分发和扩展工作流程步骤以及运行变得更加容易。 并行多个实验。

![https://storage.googleapis.com/gweb-cloudblog-publish/original_images/typical_ML_workflow.gif](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/typical_ML_workflow.gif)

[Kubeflow](https://www.kubeflow.org/) is an open source [Kubernetes](https://github.com/kubeflow/kubeflow)-native platform for developing, orchestrating, deploying, and running scalable and portable ML workloads. It helps support reproducibility and collaboration in ML workflow lifecycles, allowing you to manage end-to-end orchestration of ML pipelines, to run your workflow in multiple or hybrid environments (such as swapping between on-premises and Cloud building blocks depending upon context), and to help you reuse building blocks across different workflows. Kubeflow also provides support for visualization and collaboration in your ML workflow.
Kubeflow 是一个开源 Kubernetes 原生平台，用于开发、编排、部署和运行可扩展且可移植的 ML 工作负载。 它有助于支持 ML 工作流程生命周期中的可重复性和协作，使您能够管理 ML 管道的端到端编排，在多个或混合环境中运行工作流程（例如根据上下文在本地和云构建块之间进行交换） ，并帮助您在不同的工作流程中重复使用构建块。 Kubeflow 还为 ML 工作流程中的可视化和协作提供支持。

### Introducing Kubeflow Pipelines

[Kubeflow Pipelines](https://cloud.google.com/blog/products/ai-machine-learning/introducing-ai-hub-and-kubeflow-pipelines-making-ai-simpler-faster-and-more-useful-for-businesses) are a new component of Kubeflow that can help you compose, deploy, and manage end-to-end (optionally hybrid) machine learning workflows. Because they are a useful component of Kubeflow, they give you a no lock-in way to advance from prototyping to production. Kubeflow Pipelines also support rapid and reliable experimentation, so users can try many ML techniques to identify what works best for their application.
Kubeflow Pipelines 是 Kubeflow 的一个新组件，可以帮助您编写、部署和管理端到端（可选混合）机器学习工作流程。 因为它们是 Kubeflow 的有用组件，所以它们为您提供了一种从原型设计到生产的无锁定方式。 Kubeflow Pipelines 还支持快速可靠的实验，因此用户可以尝试多种机器学习技术来确定最适合其应用程序的技术。

In this article, we’ll describe how you can tackle ML workflow operations with [Kubeflow Pipelines](https://github.com/kubeflow/pipelines), and then we’ll highlight some [examples](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines) that you can try yourself. The examples revolve around a [TensorFlow](https://tensorflow.org/) ‘taxi fare tip prediction’ model, with data pulled from a public BigQuery dataset of [Chicago taxi trips](https://cloud.google.com/bigquery/public-data/chicago-taxi).
在本文中，我们将描述如何使用 Kubeflow Pipelines 处理 ML 工作流操作，然后我们将重点介绍一些您可以自己尝试的示例。 这些示例围绕 TensorFlow“出租车费小费预测”模型展开，数据来自芝加哥出租车行程的公共 BigQuery 数据集。

We’re running the examples on [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine/) (GKE). GKE allows easy integration of GCP services that are relevant for ML workflows, including Cloud [Dataflow](https://cloud.google.com/dataflow), [BigQuery](https://cloud.google.com/bigquery), and [Cloud ML Engine](https://cloud.google.com/ml-engine). (If you need to keep your ML workflows on-premises, you may also be interested in [GKE On-Prem](https://cloud.google.com/gke-on-prem/)ALPHA).
我们在 Google Kubernetes Engine (GKE) 上运行这些示例。 GKE 允许轻松集成与 ML 工作流程相关的 GCP 服务，包括 Cloud Dataflow、BigQuery 和 Cloud ML Engine。 （如果您需要将 ML 工作流程保留在本地，您可能也对 GKE On-PremALPHA 感兴趣）。

The examples make use of [TensorFlow Transform](https://github.com/tensorflow/transform) (TFT) for data preprocessing and to avoid training (or serving) skew, Kubeflow’s [TFJob](https://www.kubeflow.org/docs/guides/components/tftraining/) [CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) (Custom Resource Definitions library) for supporting distributed training, and [TensorFlow Model Analysis](https://github.com/tensorflow/model-analysis/) (TFMA) for analysis of learned models in conjunction with Kubeflow’s [JupyterHub](https://github.com/jupyterhub/jupyterhub) notebooks installation. These examples draw some of their code from the [example in the TFMA repo](https://github.com/tensorflow/model-analysis/tree/master/examples/chicago_taxi).
这些示例使用 TensorFlow Transform (TFT) 进行数据预处理并避免训练（或服务）偏差，使用 Kubeflow 的 TFJob CRD（自定义资源定义库）来支持分布式训练，并使用 TensorFlow 模型分析 (TFMA) 来分析学习模型 与 Kubeflow 的 JupyterHub 笔记本安装结合使用。 这些示例从 TFMA 存储库中的示例中提取了一些代码。

These workflows also include deployment of the trained models to both [Cloud ML Engine's online prediction service](https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview), and to [TensorFlow Serving](https://github.com/tensorflow/serving) via Kubeflow.
这些工作流程还包括将经过训练的模型部署到 Cloud ML Engine 的在线预测服务，以及通过 Kubeflow 部署到 TensorFlow Serving。

We’ll describe all of these components in more detail below, then show how we can *compose* and reuse these building blocks to create scalable ML workflows that exhibit consistency, reproducibility, and portability.
我们将在下面更详细地描述所有这些组件，然后展示如何组合和重用这些构建块来创建具有一致性、可重复性和可移植性的可扩展 ML 工作流程。

## Kubeflow components

The [examples](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines) highlight how Kubeflow can help support your ML lifecycle, and make it easier to support [hybrid](https://cloud.google.com/blog/products/gcp/simplifying-machine-learning-on-open-hybrid-clouds-with-kubeflow) ML solutions.
这些示例重点介绍了 Kubeflow 如何帮助支持您的 ML 生命周期，并让支持混合 ML 解决方案变得更加容易。

Kubeflow’s components include:

- Support for distributed [TensorFlow](https://tensorflow.org/) training via the [TFJob](https://www.kubeflow.org/docs/guides/components/tftraining/) [CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
  通过 TFJob CRD 支持分布式 TensorFlow 训练

- The ability to serve trained models using [TensorFlow Serving](https://github.com/tensorflow/serving)
  使用 TensorFlow Serving 为经过训练的模型提供服务的能力

- A [JupyterHub](https://github.com/jupyterhub/jupyterhub) installation with many commonly-required libraries and widgets included in the notebook installation, included those needed for [TensorFlow Model Analysis](https://github.com/tensorflow/model-analysis/) (TFMA) and [TensorFlow Transform](https://github.com/tensorflow/transform) (TFT)
  JupyterHub 安装，笔记本安装中包含许多常用的库和小部件，包括 TensorFlow 模型分析 (TFMA) 和 TensorFlow 变换 (TFT) 所需的库和小部件

- [Kubeflow Pipelines](https://github.com/kubeflow/pipelines)
  Kubeflow 管道

We use all of these in our examples, and describe them in more detail below. (Kubeflow also includes support for many other components not used in our examples.)
我们在示例中使用所有这些，并在下面更详细地描述它们。 （Kubeflow 还支持我们的示例中未使用的许多其他组件。）

## TFX building blocks

[TensorFlow Extended](https://www.tensorflow.org/tfx/) (TFX) is a TensorFlow-based platform for performant machine learning in production, first designed for use within Google, but now mostly open sourced. You can find more of an overview [here](https://dl.acm.org/citation.cfm?id=3098021).
TensorFlow Extended (TFX) 是一个基于 TensorFlow 的平台，用于在生产中实现高性能机器学习，最初设计用于 Google 内部，但现在大多开源。 您可以在此处找到更多概述。

Kubeflow and our example ML workflows use three TFX components as building blocks: [TensorFlow Transform](https://github.com/tensorflow/transform), [TensorFlow Model Analysis](https://github.com/tensorflow/model-analysis/), and [TensorFlow Serving](https://github.com/tensorflow/serving).
Kubeflow 和我们的示例 ML 工作流程使用三个 TFX 组件作为构建块：TensorFlow Transform、TensorFlow Model Analysis 和 TensorFlow Serving。

### TensorFlow Transform

TensorFlow Transform (TFT) is a library designed to preprocess data for TensorFlow—particularly for feature engineering. `tf.Transform` is useful for transformations that require a full pass of the dataset, such as normalizing an input value by mean and standard deviation, converting a vocabulary to integers by looking at all input examples for values, or categorizing inputs into buckets based on the observed data distribution.
TensorFlow Transform (TFT) 是一个旨在预处理 TensorFlow 数据的库，特别是用于特征工程。 tf.Transform 对于需要完整传递数据集的转换非常有用，例如通过均值和标准差对输入值进行标准化，通过查看值的所有输入示例将词汇表转换为整数，或者根据 观察数据分布。

Importantly, its use can also prevent [training-serving skew](https://www.coursera.org/lecture/google-machine-learning/training-and-serving-skew-C1CYm), which is a problem that occurs when the feature preprocessing run on training data is not the same as that run on new data prior to prediction. It is easy for this inconsistency to arise when training and serving are managed by different teams, often using different compute resources and different code paths.
重要的是，它的使用还可以防止训练服务偏差，当在训练数据上运行的特征预处理与预测之前在新数据上运行的特征预处理不同时，就会出现这种问题。 当训练和服务由不同的团队管理时，通常使用不同的计算资源和不同的代码路径，很容易出现这种不一致。

However, by using TFT for preprocessing, the output of `tf.Transform` is exported as a *TensorFlow graph* to use for both training and serving, and this TFT graph is exported as part of the inference graph. This process prevents training-serving skew, since the same transformations are applied in both stages. The [example pipelines](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines) use TFT to support preprocessing, and this means that after the trained models are deployed for serving and we send a prediction request, the prediction input data is being processed in exactly the same way as was done for training, without the need for any client-side preprocessing framework.
但是，通过使用 TFT 进行预处理，tf.Transform 的输出将导出为 TensorFlow 图以用于训练和服务，并且该 TFT 图将导出为推理图的一部分。 此过程可防止训练-服务偏差，因为两个阶段都应用相同的转换。 示例管道使用 TFT 来支持预处理，这意味着在部署经过训练的模型进行服务并且我们发送预测请求后，预测输入数据将以与训练完全相同的方式进行处理，而不需要 任何客户端预处理框架。

TFT uses [Apache Beam](https://beam.apache.org/) to run distributed data pipelines for analysis. Beam is an open source framework with a unified programming model for both batch and streaming use cases. Essentially, you build Beam pipelines that use the TFT transformations you want to perform.
TFT 使用 Apache Beam 运行分布式数据管道进行分析。 Beam 是一个开源框架，具有适用于批处理和流处理用例的统一编程模型。 本质上，您构建使用您想要执行的 TFT 转换的 Beam 管道。

Beam allows you to run your workloads on a choice of different execution engines, including a local runner, and [Google Cloud Dataflow](https://cloud.google.com/dataflow) (Google Cloud’s managed service for running Beam pipelines). By default, the example pipelines use Beam’s local runner, but can transparently use Cloud Dataflow instead, by setting a configuration parameter. (For these examples, the default datasets are small, so running locally works fine, but for processing larger datasets, Cloud Dataflow lets you automatically scale out your processing across multiple workers.)
Beam 允许您在选择的不同执行引擎上运行工作负载，包括本地运行器和 Google Cloud Dataflow（Google Cloud 用于运行 Beam 管道的托管服务）。 默认情况下，示例管道使用 Beam 的本地运行器，但可以通过设置配置参数来透明地使用 Cloud Dataflow。 （对于这些示例，默认数据集很小，因此在本地运行效果很好，但对于处理较大的数据集，Cloud Dataflow 可让您自动跨多个工作人员扩展处理。）

### TensorFlow Model Analysis (and JupyterHub on Kubeflow)

The second TFX component used in the example workflows is [TensorFlow Model Analysis](https://github.com/tensorflow/model-analysis/) (TFMA). TFMA is a library for evaluating TensorFlow models. It allows users to evaluate their models on large amounts of data in a distributed manner, using the same metrics defined for training. These metrics can be computed over different slices of data and visualized in Jupyter notebooks.
示例工作流程中使用的第二个 TFX 组件是 TensorFlow 模型分析 (TFMA)。 TFMA 是一个用于评估 TensorFlow 模型的库。 它允许用户使用为训练定义的相同指标以分布式方式评估大量数据的模型。 这些指标可以通过不同的数据片段进行计算，并在 Jupyter 笔记本中进行可视化。

TFMA makes it easy to visualize the performance of a model across a range of circumstances, features, and subsets of its user population, helping to give developers the analytic insights they need to be confident their models will [treat all users fairly](https://cloud.google.com/blog/products/ai-machine-learning/steering-the-right-course-for-ai).
TFMA 可以轻松可视化模型在各种环境、特征和用户群体子集下的性能，有助于为开发人员提供所需的分析见解，以确保他们的模型将公平对待所有用户。

![https://storage.googleapis.com/gweb-cloudblog-publish/original_images/image7.gif](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/image7.gif)

You can compute slices of interest as part of your main ML workflow, so that the results are ready to examine in a notebook environment—just as we demonstrate in these example workflows. Kubflow includes a [JupyterHub](https://github.com/jupyterhub/jupyterhub) installation with the necessary TFMA libraries and widgets installed, and this makes it very straightforward to explore the analysis results in a Kubeflow-supported Jupyter notebook. (Other TFX libraries are also installed, along with many other libraries useful to data scientists.)
您可以计算感兴趣的切片作为主要 ML 工作流程的一部分，以便可以在笔记本环境中检查结果 - 正如我们在这些示例工作流程中演示的那样。 Kubflow 包括一个 JupyterHub 安装，其中安装了必要的 TFMA 库和小部件，这使得在 Kubeflow 支持的 Jupyter Notebook 中探索分析结果变得非常简单。 （还安装了其他 TFX 库，以及许多对数据科学家有用的其他库。）

As with TFT, Apache Beam is required to run distributed analysis, and just as with TFT, the example workflows support use of the Beam local runner, and can also be run on Cloud Dataflow. For the small example datasets, running Beam locally works fine.
与 TFT 一样，需要 Apache Beam 来运行分布式分析，并且与 TFT 一样，示例工作流支持使用 Beam 本地运行程序，也可以在 Cloud Dataflow 上运行。 对于小型示例数据集，在本地运行 Beam 效果很好。

### TensorFlow Serving

[TensorFlow Serving](https://github.com/tensorflow/serving) (often abbreviated as “TF-Serving”) is another TFX component, consisting of an open source library, binaries, and images for serving machine learning models. It deals with the *inference* aspect of machine learning, managing and serving trained models, and can even help if you need to serve your TensorFlow models on-prem. TensorFlow Serving is [a Kubeflow core component](https://www.kubeflow.org/docs/guides/components/tfserving/), which means that it is installed by default when you deploy Kubeflow. [One example](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines/samples/kubeflow-tf/README.md#example-workflow-1) shows how to deploy trained models to TF-Serving, and the example repo includes a client script that lets you make requests to the deployed TF-Serving models.
TensorFlow Serving（通常缩写为“TF-Serving”）是另一个 TFX 组件，由开源库、二进制文件和用于服务机器学习模型的图像组成。 它涉及机器学习的推理方面、管理和服务经过训练的模型，甚至可以在您需要在本地提供 TensorFlow 模型时提供帮助。 TensorFlow Serving 是 Kubeflow 核心组件，这意味着在部署 Kubeflow 时默认安装它。 一个示例展示了如何将经过训练的模型部署到 TF-Serving，示例存储库包含一个客户端脚本，可让您向已部署的 TF-Serving 模型发出请求。

### TensorFlow Data Validation

[TensorFlow Data Validation](https://github.com/tensorflow/data-validation) (TFDV) is a library for exploring and validating machine learning data. It is designed to be highly scalable and to work well with TensorFlow and TensorFlow Extended (TFX). Its features include automated [data schema](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto) generation to describe expectations about data like required values, ranges, and vocabularies. While the example pipelines do not invoke TFDV directly, the schema used by the TFT and TFMA pipeline steps was generated by TFDV.
TensorFlow 数据验证 (TFDV) 是一个用于探索和验证机器学习数据的库。 它的设计具有高度可扩展性，并且能够与 TensorFlow 和 TensorFlow Extended (TFX) 良好配合。 其功能包括自动生成数据模式，以描述对所需值、范围和词汇等数据的期望。 虽然示例管道不直接调用 TFDV，但 TFT 和 TFMA 管道步骤使用的架构是由 TFDV 生成的。

## Cloud ML Engine Online Prediction

[Google Cloud ML Engine](https://cloud.google.com/ml-engine/docs/) is a managed service for training and serving ML models: not only TensorFlow, but scikit-learn and XGBoost as well. Cloud ML Engine makes it easy to do distributed training and scalable serving, and it provides monitoring, logging, and model version management. You can use Cloud ML Engine services as key building blocks for many ML workflows.
Google Cloud ML Engine 是一项用于训练和服务 ML 模型的托管服务：不仅包括 TensorFlow，还包括 scikit-learn 和 XGBoost。 Cloud ML Engine 可以轻松进行分布式训练和可扩展服务，并提供监控、日志记录和模型版本管理。 您可以使用 Cloud ML Engine 服务作为许多 ML 工作流程的关键构建块。

For the examples described in this post, since we’re highlighting Kubeflow’s [TFJob](https://www.kubeflow.org/docs/guides/components/tftraining/) API, we’re not using Cloud ML Engine for training (though we could). However, we’re deploying the trained TensorFlow models to [Cloud ML Engine online prediction service](https://cloud.google.com/ml-engine/docs/tensorflow/online-predict), which provides scalable serving—for example, if you’re accessing a model via an app you built, and the app becomes popular, you have no worries about your model serving infrastructure falling over when it gets a barrage of inbound requests.
对于本文中描述的示例，由于我们重点介绍 Kubeflow 的 TFJob API，因此我们没有使用 Cloud ML Engine 进行训练（尽管我们可以）。 不过，我们正在将经过训练的 TensorFlow 模型部署到 Cloud ML Engine 在线预测服务，该服务提供可扩展的服务 - 例如，如果您通过自己构建的应用程序访问模型，并且该应用程序变得流行，那么您无需担心 当您的模型服务基础设施收到大量入站请求时，就会崩溃。

The workflow deploys the trained models as *versions* of a specific model, in this case named `taxifare`. Once the model versions are deployed, we can make prediction requests against a specific version. The [Google Cloud Platform Console](https://console.cloud.google.com/mlengine/models) lets you browse through the deployed versions of your different models, set one to be the default, and get information about when each was deployed and last accessed.
该工作流程将经过训练的模型部署为特定模型的版本，在本例中名为“taxifare”。 部署模型版本后，我们可以针对特定版本发出预测请求。 通过 Google Cloud Platform Console，您可以浏览不同模型的已部署版本，将其设置为默认版本，并获取有关每个模型的部署时间和上次访问时间的信息。

![https://storage.googleapis.com/gweb-cloudblog-publish/images/image3_x2bnuT9.max-1257x706.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/image3_x2bnuT9.max-1257x706.png)

As mentioned above, our workflows use TensorFlow Model Analysis to analyze and compare the learned models, and we can use that information to select the best version as the default, the version served when you make an API request using just the `taxifare` model name. You can set the default version from the GCP Console, or via the [gcloud](https://cloud.google.com/sdk/) command line utility.
如上所述，我们的工作流程使用 TensorFlow 模型分析来分析和比较学习到的模型，我们可以使用该信息选择最佳版本作为默认版本，即您仅使用出租车模型名称发出 API 请求时提供的版本。 您可以从 GCP Console 或通过 gcloud 命令行实用程序设置默认版本。

## Building workflows using Kubeflow Pipelines

The building blocks described above can be composed to support common and useful ML workflow patterns. They let you build pipelines that support **data ingestion, feature pre-processing, distributed training, evaluation, and serving**. Our [examples](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines) show variants on this basic workflow, and illustrate how easy it is to create these variants via reusable building blocks. In this section, we’ll take a closer look at one of them.
可以组合上述构建块来支持常见且有用的 ML 工作流程模式。 它们可让您构建支持数据摄取、特征预处理、分布式训练、评估和服务的管道。 我们的示例展示了此基本工作流程的变体，并说明了通过可重用构建块创建这些变体是多么容易。 在本节中，我们将仔细研究其中之一。

### Constructing a workflow using the Kubeflow Pipelines SDK

The [Pipelines SDK](https://github.com/kubeflow/pipelines/wiki) lets you specify your ML workflows in a high-level language. Then, you compile and run your specifications.
Pipelines SDK 允许您以高级语言指定 ML 工作流程。 然后，您编译并运行您的规范。

Our [first example workflow](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/samples/kubeflow-tf/workflow1.py) illustrates how you can use an ML workflow to experiment with [TFT](https://github.com/tensorflow/transform)-based feature engineering, and how you can support a hybrid ML flow that serves your trained model from both on-prem and cloud endpoints.
我们的第一个示例工作流程说明了如何使用 ML 工作流程来试验基于 TFT 的特征工程，以及如何支持混合 ML 流程，以从本地和云端点为您的训练模型提供服务。

The workflow runs two paths concurrently, using a different *TFT preprocessing* function for each path ([preprocessing.py](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines/components/dataflow/tft/preprocessing.py) vs. [preprocessing2.py](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines/components/dataflow/tft/preprocessing2.py)). By designing the TFT workflow component to take the preprocessing function definition as an argument, it effectively becomes a reusable component that you can incorporate into any pipeline.
该工作流程同时运行两个路径，为每个路径使用不同的 TFT 预处理函数（preprocessing.py 与 preprocessing2.py）。 通过设计 TFT 工作流组件以将预处理函数定义作为参数，它实际上成为一个可重用的组件，您可以将其合并到任何管道中。

Then you can train each model variant, using Kubeflow’s [TFJob](https://www.kubeflow.org/docs/guides/components/tftraining/) [CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/). For example purposes, *distributed training* is used for one path, leveraging TFJob’s support for easy distribution, and single-node training is used for the other. This distinction is made by specifying the number of *workers* and parameter servers to use for the training job.
然后，您可以使用 Kubeflow 的 TFJob CRD 训练每个模型变体。 例如，一条路径使用分布式训练，利用 TFJob 对轻松分发的支持，另一条路径使用单节点训练。 这种区别是通过指定用于训练作业的工作人员和参数服务器的数量来实现的。

Then, the workflow runs TFMA analysis on both trained models, so that they can be evaluated and compared, and at the same time deploys the trained models to *both* Cloud ML Engine and TF-Serving. With the use of TFT, the deployed models include the TFT-generated preprocessing graphs, so we don’t have to worry about training or serving skew, and by using modular building blocks, it becomes much easier to assemble experiments like these.
然后，工作流程对两个经过训练的模型运行 TFMA 分析，以便对它们进行评估和比较，同时将经过训练的模型部署到 Cloud ML Engine 和 TF-Serving。 通过使用 TFT，部署的模型包括 TFT 生成的预处理图，因此我们不必担心训练或服务偏差，并且通过使用模块化构建块，组装此类实验变得更加容易。

This example shows how you can support hybrid workflows, where, for example, your training runs on-premises (maybe you have some sensitive training data), and then you deploy to both your on-prem TensorFlow Serving cluster and Cloud ML Engine Online Prediction. It also shows how easy it is to scale out a Kubeflow TensorFlow training job, from a single-node to a large distributed cluster, by merely changing the job parameters.
此示例展示了如何支持混合工作流程，例如，您的训练在本地运行（也许您有一些敏感的训练数据），然后部署到本地 TensorFlow Serving 集群和 Cloud ML Engine Online Prediction 。 它还展示了只需更改作业参数即可轻松地将 Kubeflow TensorFlow 训练作业从单节点扩展到大型分布式集群是多么容易。

The workflow graph looks like this:

![https://storage.googleapis.com/gweb-cloudblog-publish/images/workflow1_graph_ds.max-949x534.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/workflow1_graph_ds.max-949x534.png)

We’ll build this pipeline using the [Kubeflow Pipelines SDK](https://github.com/kubeflow/pipelines) as follows. First, we define the workflow’s input parameters and defaults:

```python
import kfp.dsl as dsl
import kfp.gcp as gcp


@dsl.pipeline(
  name='Workflow 1',
  description='Demonstrate TFT-based feature processing, TFMA, TFJob, CMLE OP, and TF-Serving'
)
def workflow1(
  input_handle_eval: dsl.PipelineParam=dsl.PipelineParam(name='input-handle-eval', value='gs://aju-dev-demos-codelabs/KF/taxidata/eval/data.csv'),
  input_handle_train: dsl.PipelineParam=dsl.PipelineParam(name='input-handle-train', value='gs://aju-dev-demos-codelabs/KF/taxidata/train/data.csv'),
  outfile_prefix_eval: dsl.PipelineParam=dsl.PipelineParam(name='outfile-prefix-eval', value='eval_transformed'),
  outfile_prefix_train: dsl.PipelineParam=dsl.PipelineParam(name='outfile-prefix-train', value='train_transformed'),
  train_steps: dsl.PipelineParam=dsl.PipelineParam(name='train-steps', value=10000),
  project: dsl.PipelineParam=dsl.PipelineParam(name='project', value='YOUR_PROJECT_HERE'),
  working_dir: dsl.PipelineParam=dsl.PipelineParam(name='working-dir', value='YOUR_GCS_DIR_HERE'),
  tft_setup_file: dsl.PipelineParam=dsl.PipelineParam(name='tft-setup-file', value='/ml/transform/setup.py'),
  tfma_setup_file: dsl.PipelineParam=dsl.PipelineParam(name='tfma-setup-file', value='/ml/analysis/setup.py'),
  workers: dsl.PipelineParam=dsl.PipelineParam(name='workers', value=1),
  pss: dsl.PipelineParam=dsl.PipelineParam(name='pss', value=1),
  max_rows: dsl.PipelineParam=dsl.PipelineParam(name='max-rows', value=10000),
  ts1: dsl.PipelineParam=dsl.PipelineParam(name='ts1', value=''),
  ts2: dsl.PipelineParam=dsl.PipelineParam(name='ts2', value=''),
  preprocessing_module1: dsl.PipelineParam=dsl.PipelineParam(name='preprocessing-module1', value='gs://aju-dev-demos-codelabs/KF/taxi-preproc/preprocessing.py'),
  preprocessing_module2: dsl.PipelineParam=dsl.PipelineParam(name='preprocessing-module2', value='gs://aju-dev-demos-codelabs/KF/taxi-preproc/preprocessing2.py'),
  preprocess_mode: dsl.PipelineParam=dsl.PipelineParam(name='preprocess-mode', value='local'),
  tfma_mode: dsl.PipelineParam=dsl.PipelineParam(name='tfma-mode', value='local')):

```

Next, we define the workflow’s component steps and their dependencies. For each component, we’re specifying a Docker container image, and the arguments to pass to the container’s endpoint. While not used here, you can also override the container endpoint by specifying a command to run, or specify output files written during the component’s execution. We will very soon be providing utilities that will, in many cases, eliminate the need for you to worry about Docker container creation. You will just provide your code and specify a base image to use, and Pipelines will take care of the rest.
接下来，我们定义工作流程的组件步骤及其依赖关系。 对于每个组件，我们指定一个 Docker 容器映像，以及要传递到容器端点的参数。 虽然此处未使用，但您还可以通过指定要运行的命令来覆盖容器端点，或指定在组件执行期间写入的输出文件。 我们很快将提供实用程序，在许多情况下，这些实用程序将消除您对 Docker 容器创建的担忧。 您只需提供代码并指定要使用的基本映像，管道将处理其余的事情。

We can now specify dependencies between the steps with the `op.after()` construct. See the [Kubeflow Pipelines documentation](https://github.com/kubeflow/pipelines/wiki) for more explanation. For brevity, we omit detail for most of the step definitions, but the full Python script is [here](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/samples/kubeflow-tf/workflow1.py).
我们现在可以使用 op.after() 构造指定步骤之间的依赖关系。 有关更多说明，请参阅 Kubeflow Pipelines 文档。 为简洁起见，我们省略了大多数步骤定义的详细信息，但完整的 Python 脚本位于此处。

We’ll first define the TensorFlow Transform steps. There are four in total: two for each processing pipeline, to process train and evaluate data. We’re not specifying any dependencies between them, so they all run concurrently:
我们首先定义 TensorFlow 变换步骤。 总共有四个：每个处理管道两个，用于处理训练和评估数据。 我们没有指定它们之间的任何依赖关系，因此它们都同时运行：

```python
tfteval = dsl.ContainerOp(
      name = 'tft-eval',
      image = 'gcr.io/google-samples/ml-pipeline-dataflow-tftbq-taxi',
      arguments = [ "--input_handle", input_handle_eval, 
          "--outfile_prefix", outfile_prefix_eval,
          "--working_dir", '%s/%s/tft-eval' % (working_dir, '{{workflow.name}}'),
          "--project", project,
          "--mode", preprocess_mode,
          "--setup_file", tft_setup_file,
          "--max-rows", 5000,
          "--ts1", ts1,
          "--ts2", ts2,
          "--stage", "eval",
          "--preprocessing-module", preprocessing_module1]
      # file_outputs = {'transformed': '/output.txt'}
      )
  tfttrain = ...
  tfteval2 = ...
  tfttrain2 = ...
```

Next, we’ll define the ‘train’ steps. There are two, one for each TFT feature engineering variant. We’ll require that they run `after` their respective TFT components:

```python
train = ...
  train.after(tfteval)
  train.after(tfttrain)

  train2 = ...
  train2.after(tfteval2)
  train2.after(tfttrain2)
```

After the models are trained, we will run TFMA-based analysis on the results, and deploy the trained models to both the [Cloud ML Engine online prediction service](https://cloud.google.com/ml-engine/docs/tensorflow/online-predict), and to TensorFlow serving. We again use the `op.after()` construct to indicate that all of these activities can happen concurrently after training of each model has finished.

```python
analyze = ...
  analyze2 = ...

  cmleop = ...
  cmleop2 = ...

  tfserving = ...
  tfserving2 = ...

  analyze.after(train)
  analyze2.after(train2)
  cmleop.after(train)
  cmleop2.after(train2)
  tfserving.after(train)
  tfserving2.after(train2)
```

See the Pipelines repo for [more examples](https://github.com/kubeflow/pipelines/tree/master/samples). It is also possible to author and deploy pipelines and pipeline components from within a notebook. We’ll describe that in a follow-on blog post.
有关更多示例，请参阅 Pipelines 存储库。 还可以在笔记本中创作和部署管道和管道组件。 我们将在后续博客文章中对此进行描述。

### Monitoring a workflow using the Pipelines UI

The Kubeflow Pipelines UI (user interface) provides support for monitoring and inspecting Pipeline specifications, *experiments* based on a given Pipeline, and multiple *runs* of an experiment. A specification like the one above is *compiled*, then uploaded via the UI.
Kubeflow Pipelines UI（用户界面）支持监视和检查 Pipeline 规范、基于给定 Pipeline 的实验以及实验的多次运行。 编译像上面这样的规范，然后通过 UI 上传。

After you’ve uploaded a pipeline definition, you can view the pipeline graph derived from your specification. (For pipelines with dynamically-generated steps, this initial graph will be refined at run-time). Then, you can start experiments based on that pipeline, and initiate or schedule multiple experimental runs.
上传管道定义后，您可以查看从规范派生的管道图。 （对于具有动态生成步骤的管道，该初始图将在运行时进行细化）。 然后，您可以根据该管道开始实验，并启动或安排多个实验运行。

![https://storage.googleapis.com/gweb-cloudblog-publish/images/wkflow1_static_graph_ds.max-1191x670.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/wkflow1_static_graph_ds.max-1191x670.png)

While an experiment run is in progress, or after it has finished, you can inspect the dynamically-generated graph, configuration parameters, and logs for the pipeline steps.

![https://storage.googleapis.com/gweb-cloudblog-publish/images/Screenshot_2018-11-04_14_47_00_ds.max-1212x682.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/Screenshot_2018-11-04_14_47_00_ds.max-1212x682.png)

![https://storage.googleapis.com/gweb-cloudblog-publish/images/Screenshot_2018-11-04_14_51_32_ds.max-956x538.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/Screenshot_2018-11-04_14_51_32_ds.max-956x538.png)

### Use TensorBoard from the Kubeflow Pipelines UI

[TensorBoard](https://github.com/tensorflow/tensorboard) is a suite of web applications that help you [inspect and understand](https://www.tensorflow.org/guide/summaries_and_tensorboard) your TensorFlow runs and graphs. If your pipeline includes TensorFlow training components, you can define these components to write metadata that indicates the location of output consumable by TensorBoard. The Pipelines UI uses this information to launch TensorBoard servers from the UI.
TensorBoard 是一套 Web 应用程序，可帮助您检查和了解 TensorFlow 运行和图表。 如果您的管道包含 TensorFlow 训练组件，您可以定义这些组件来写入指示 TensorBoard 输出消耗品位置的元数据。 Pipelines UI 使用此信息从 UI 启动 TensorBoard 服务器。

![https://storage.googleapis.com/gweb-cloudblog-publish/images/start_tensorboard_ds.max-1004x532.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/start_tensorboard_ds.max-1004x532.png)

Here, we’re using TensorBoard to view the results of one of the training steps in the workflow above.

![https://storage.googleapis.com/gweb-cloudblog-publish/images/view_tb_ds.max-783x440.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/view_tb_ds.max-783x440.png)

### Use Kubeflow to visualize model analysis results in a Jupyter notebook

In our example workflows, we run TensorFlow Model Analysis (TFMA) on the trained models, using a provided [specification of how to slice the data](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines/components/dataflow/tfma/model_analysis-taxi.py#L45).
在我们的示例工作流程中，我们使用提供的有关如何切片数据的规范在经过训练的模型上运行 TensorFlow 模型分析 (TFMA)。

At any time after this workflow step has been run, you can visualize the TFMA results in a Jupyter notebook, making it easy to assess model quality or compare models.
运行此工作流程步骤后，您可以随时在 Jupyter 笔记本中可视化 TFMA 结果，从而轻松评估模型质量或比较模型。

Kubeflow’s [JupyterHub](https://github.com/jupyterhub/jupyterhub) installation makes this easy to do, via a `port-forward` to your Kubernetes Engine (GKE) cluster. The necessary libraries and visualization widgets are already installed. If you’re playing along, see the instructions [here](https://www.kubeflow.org/docs/guides/components/jupyter/) on connecting to JupyterHub via the Kubeflow Dashboard. Then load and run the [tfma_expers.ipynb](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines/components/dataflow/tfma/tfma_expers.ipynb) notebook to explore the results of your TFMA analysis.
Kubeflow 的 JupyterHub 安装可以通过端口转发到 Kubernetes Engine (GKE) 集群来轻松完成此操作。 必要的库和可视化小部件已经安装。 如果您正在尝试，请参阅此处有关通过 Kubeflow 仪表板连接到 JupyterHub 的说明。 然后加载并运行 tfma_expers.ipynb 笔记本以探索 TFMA 分析的结果。

## Running the example workflows

To run the example workflows yourself, see the [README](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines/README.md), which walks you through the necessary installation steps and describes the code in more detail.

Above, we showed `Workflow 1`. We’ll take a quick look here at `Workflow 2`, which uses the same workflow components, but combines them in a different way.

### Workflow 2

[Workflow 2](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/samples/kubeflow-tf/workflow2.py) shows how you might use TFMA to investigate relative accuracies of models trained on different datasets, evaluating against ‘new’ data. As part of the preprocessing step, it pulls data directly from the source [BigQuery Chicago taxi dataset](https://cloud.google.com/bigquery/public-data/chicago-taxi), with differing `min` and `max` time boundaries, effectively training on ‘recent’ data vs a batch that includes older data. Then, it runs TFMA analysis on both learned models, using the newest data for evaluation.

![https://storage.googleapis.com/gweb-cloudblog-publish/images/wkflw2_graph_ds.max-1165x655.png](https://storage.googleapis.com/gweb-cloudblog-publish/images/wkflw2_graph_ds.max-1165x655.png)

As with `Workflow 1` above, the trained models are deployed to Cloud ML Engine Online Prediction, where you can then select the most accurate ones to use for prediction.

This example shows how you can define workflows to support consistent model regeneration and re-evaluation over sliding time windows of your data, to determine whether the characteristics of your new prediction data have changed. (While not shown as part of this example, you could alternatively use a similar workflow to support incremental training of an existing model on successive new datasets, and then compare that model with new models trained ‘from scratch.’)

The Pipelines SDK specification for `Workflow 2` is [here](https://github.com/amygdala/code-snippets/blob/master/ml/kubeflow-pipelines/samples/kubeflow-tf/workflow2.py).

## Use your models for online prediction with Cloud ML Engine

As part of both example workflows, the trained models are deployed to Cloud ML Engine’s [online prediction service](https://cloud.google.com/ml-engine/docs/tensorflow/online-predict). The model name is `taxifare`, and the version names are derived from the workflow names. As shown above, you can view the deployed versions of the `taxifare` model in the [GCP Console](https://console.cloud.google.com/mlengine/models).

If you’re following along, it is easy to *make a prediction using one of the deployed Cloud ML Engine model versions.* Follow the instructions in [this section of the README](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines#use-your-models-for-prediction-with-cloud-ml-engine-online-prediction), then run the following command, replacing `<CMLE_MODEL_VERSION_NAME>`.

```bash
python chicago_taxi_client.py \
  --num_examples=1 \
  --examples_file='../taxi_model/data/eval/data.csv' \
  --server=mlengine:taxifare --model_name=<CMLE_MODEL_VERSION_NAME>
```

(This command requires that you have the [gcloud sdk installed](https://cloud.google.com/sdk/install), or as described in the [README](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines/README.md), you can use your project’s [Cloud Shell](https://cloud.google.com/shell/docs/) instead).

## Make predictions using TF-Serving endpoints

The first example pipeline deployed the trained models not only to Cloud ML Engine, but also to [TensorFlow Serving](https://github.com/tensorflow/serving), which is part of the Kubeflow installation.

To facilitate a simpler demo, the TF-Serving deployments use a Kubernetes service of type `LoadBalancer`, which creates an endpoint with an external IP. However, for a production system, you’d probably want to use something like [Cloud Identity-Aware Proxy](https://cloud.google.com/iap/).

You can view the TF-Serving endpoint services created by the pipeline by running:

```bash
kubectl get services -n kubeflow
```

from the command line. For this particular pipeline, look for the services with prefix `workflow1` (its prefix), and note their names and external IP addresses.

It is easy to make requests to the TensorFlow Serving endpoints using a client script: you can find more detail in the [README](https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines#access-the-tf-serving-endpoints-for-your-learned-model).

Should you want to scale a TF-Serving endpoint to handle a large number of requests, this is easy to do via Kubernetes' underlying capabilities: scale the [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) backing the endpoint service.

## Learn or contribute

We hope these examples encourage you try out Kubeflow and Kubeflow Pipelines yourself, and even become a contributor. Here are some resources for learning more and getting help:

- More [Kubeflow examples](https://github.com/kubeflow/examples)

- [Kubeflow’s Slack channel](https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg)

- The [Kubeflow-discuss email list](https://groups.google.com/forum/#!forum/kubeflow-discuss)

- [Kubeflow’s Twitter account](http://twitter.com/kubeflow)

- Kubeflow’s [weekly community meeting](https://github.com/kubeflow/community)

- More [Kubeflow Pipelines examples](https://github.com/kubeflow/pipelines/tree/master/samples)

- Kubeflow Pipelines [documentation](https://github.com/kubeflow/pipelines/wiki)

Please submit bugs and tell us what features you’d like to see!

